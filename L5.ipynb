{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "YhhrPjx7oVTZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lesson5 - On Deep Neural Network Basics"
      ]
    },
    {
      "metadata": {
        "id": "7TbaaJheouh7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 - Numpy Basics\n",
        "`numpy` is a handy python module for scientific computing and especially matrix computing.  \n",
        "Usually we rename module `numpy` as `np`"
      ]
    },
    {
      "metadata": {
        "id": "UIYzqmMNovE8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from IPython.display import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tj8KWQ_1tUC7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 - Fuctions"
      ]
    },
    {
      "metadata": {
        "id": "F0MCSDm6pbaG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`np.array(list)` is often used to create numpy array from list."
      ]
    },
    {
      "metadata": {
        "id": "GVT5mzsUpHna",
        "colab_type": "code",
        "outputId": "d7dc12af-a89d-4031-d53a-19297cb7ed5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "arr = np.array([1, 2, 3])\n",
        "print(arr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nHW8rNedp5P5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`np.zeros(arr_shape)` returns a matrix or vector in shape ordered by tuple `arr_shape`.  \n",
        "**Attention: arr_shape is a tuple**"
      ]
    },
    {
      "metadata": {
        "id": "zxHFdV49qTYH",
        "colab_type": "code",
        "outputId": "4001fcb1-3a78-4207-8720-31c0dbcfd170",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "zero = np.zeros((2, 3))\n",
        "print(zero)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7gOd3U4ZqdMi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`np_matrix.shape` is the numpy matirx's shape in a tuple."
      ]
    },
    {
      "metadata": {
        "id": "0-IsS92gqu5Z",
        "colab_type": "code",
        "outputId": "44dbb83a-150e-4ec1-d7d5-7c1655aaf60f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(zero.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2d6PrbtWri0Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Pay attention, when you create numpy matrix from a one dimension list, the shape is usually `(m, )`.  \n",
        "Usually we reshape the matirx to be `(m, 1)` in case causing annoying bugs.\n",
        "The fuction you'll need is `np_matrix.reshape(dim1, dim2,..., dimn)`"
      ]
    },
    {
      "metadata": {
        "id": "g5FkCLHksBJ0",
        "colab_type": "code",
        "outputId": "2063ccfd-b0ac-470d-c570-26c34a494639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print(arr.shape)\n",
        "arr = arr.reshape(arr.shape[0], 1)\n",
        "print(arr.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3,)\n",
            "(3, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0l90g7J0rCmV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`np.random.rand(dim1, dim2, ..., dimn)` returns a random initialized matrix shaped as tuple `arr_shape` appointed."
      ]
    },
    {
      "metadata": {
        "id": "J0cKRYc5s2os",
        "colab_type": "code",
        "outputId": "15a769b6-fa5e-48bf-dd76-797c5947c839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "mat_rand = np.random.rand(3, 5)\n",
        "print(mat_rand)\n",
        "print(mat_rand.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.47602897 0.88076377 0.01247704 0.3980761  0.98403803]\n",
            " [0.63483988 0.37220466 0.53616491 0.76325326 0.94196743]\n",
            " [0.39335472 0.23892841 0.94801984 0.03017083 0.9555027 ]]\n",
            "(3, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9k7m2zrHuGDy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`np.sum(matrix, axis=p, keepdims=True/False)` means to add matrix in dimension p, and returns."
      ]
    },
    {
      "metadata": {
        "id": "M9lSfgoVuiqp",
        "colab_type": "code",
        "outputId": "246c13b5-d18f-4261-8955-93e25b83f998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "cell_type": "code",
      "source": [
        "rand_sum = np.sum(mat_rand, axis=1, keepdims=True)\n",
        "print(rand_sum)\n",
        "print(rand_sum.shape)\n",
        "rand_sum = np.sum(mat_rand, axis=1, keepdims=False)\n",
        "print(rand_sum)\n",
        "print(rand_sum.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.75138391]\n",
            " [3.24843014]\n",
            " [2.56597651]]\n",
            "(3, 1)\n",
            "[2.75138391 3.24843014 2.56597651]\n",
            "(3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8S0cefTpPIuW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "`np.dot(m1, m2)` does the matrix multiplication. Tell it apart from elemental multiplication."
      ]
    },
    {
      "metadata": {
        "id": "MtXwOdumPW0w",
        "colab_type": "code",
        "outputId": "e81e5bdf-bd47-4eec-c863-6b2ccc4b63b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "A = np.ones((3, 3))\n",
        "print(A)\n",
        "I = np.eye(3, 3)\n",
        "print(I)\n",
        "print(np.dot(A, I))\n",
        "print(A * I)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n",
            "[[1. 1. 1.]\n",
            " [1. 1. 1.]\n",
            " [1. 1. 1.]]\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5i0uGANttYrM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2 - Broadcasting"
      ]
    },
    {
      "metadata": {
        "id": "vwumZWt-ta2k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In numpy matrix, there is a concept called **broadcasting**, which allows us to add, multiply and divide matrix with matrix, vector and scalar in different shape."
      ]
    },
    {
      "metadata": {
        "id": "RccPFE_lwPeW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* matrix op matrix"
      ]
    },
    {
      "metadata": {
        "id": "hp_DWSpwtaCX",
        "colab_type": "code",
        "outputId": "7ac6ea2d-89e5-4114-f5a6-2fdcc8868fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "a = np.random.rand(3, 3)\n",
        "print(a)\n",
        "b = np.ones((3, 1))\n",
        "print(b)\n",
        "print(a + b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.3478792  0.69534942 0.13370806]\n",
            " [0.04935268 0.88896422 0.07573869]\n",
            " [0.79059463 0.44352386 0.10042051]]\n",
            "[[1.]\n",
            " [1.]\n",
            " [1.]]\n",
            "[[1.3478792  1.69534942 1.13370806]\n",
            " [1.04935268 1.88896422 1.07573869]\n",
            " [1.79059463 1.44352386 1.10042051]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eKm10N3awvik",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* matrix op scalar"
      ]
    },
    {
      "metadata": {
        "id": "pWcM_hQHwe7n",
        "colab_type": "code",
        "outputId": "ad7cd783-d957-4d6f-b040-9ff5ce710d4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "print(a + 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.3478792  1.69534942 1.13370806]\n",
            " [1.04935268 1.88896422 1.07573869]\n",
            " [1.79059463 1.44352386 1.10042051]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Pnf3CJ-w5PN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* matrix op vector"
      ]
    },
    {
      "metadata": {
        "id": "yqxtLFK8wmrm",
        "colab_type": "code",
        "outputId": "a7b36495-fdb8-4745-d223-f47eb6984888",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "b = np.ones((3, ))\n",
        "print(a + b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.3478792  1.69534942 1.13370806]\n",
            " [1.04935268 1.88896422 1.07573869]\n",
            " [1.79059463 1.44352386 1.10042051]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aEGQRj5qBwRH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 - Logistic Regression  \n",
        "There is a `binary classification` problem to recognize cat. "
      ]
    },
    {
      "metadata": {
        "id": "7Z4FEgRxUg4A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 - Data Normalization  \n",
        "Usually we map the pixel value of image to `(-1, 1)` or `(0, 1)`before training the model.  \n",
        "It makes gradient descent to converge faster.  \n",
        "![Normalization](https://drive.google.com/uc?id=1t4M1azqisIrk4xTQEgvy1Tz3M-0szRJd)"
      ]
    },
    {
      "metadata": {
        "id": "6Z1-y12OYlTR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 - Defining Structure  \n",
        "![LogReg_kiank.png](https://drive.google.com/uc?id=1jO7mU5iz2-XAF9F_zwoAanNG9o5-Qxx3)\n"
      ]
    },
    {
      "metadata": {
        "id": "kbekKjmoW9BT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3 - Parameters Initialization . \n",
        "Usually we initialize parameters $W$ randomly, and let $b$ to be zero.  \n",
        "Question: Why don't initialize all parameters to be zero?\n"
      ]
    },
    {
      "metadata": {
        "id": "K7LV0cnNI0xs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.4 - Forward Propagation\n",
        "The original image is an 64 * 64 RGB, which means the shape is `(64, 64, 3)`.     \n",
        "$x^{(i)}$ is the $i$th image  shaped as a colomn vector.    \n",
        "So $x^{(i)}.shape = $   `(12288, 1)`    \n",
        "The output is the probability of image being a cat.  \n",
        "We suppose the probability could be calculated by a linear funtion:  \n",
        "$$z^{(i)} = w_0x^{(i)}_0 + w_1x^{(i)}_1 + ... + w_{12287}x^{(i)}_{12287} + b$$    \n",
        "But $y^{(i)}$ maybe too large, so we use fuction `sigmoid` to map $y^{(i)}$ to `(0, 1)`    \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3MWGLPmfTXs2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![sigmoid](https://drive.google.com/uc?id=1BIBcyF9SFut2o3Z9nur27UnSaDgMBoXP)"
      ]
    },
    {
      "metadata": {
        "id": "E7GPzL319BjE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So the predicted probability of image being cat is:\n",
        "$$\\hat y^{(i)} = a^{(i)} = sigmoid(z^{(i)}) = \\frac{1}{1 + e^{-z}}$$  \n",
        "We usually call the output of a neuron **activation**, the reason would be discussed later."
      ]
    },
    {
      "metadata": {
        "id": "hgsRzpAjF8Aw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.4.1 Vectorization\n",
        "How do we deal with multiple images at the same time?  \n",
        "It's called vectorization, mainly depends on matrix operations.  \n",
        "We use $X = (x^{(1)}, x^{(2)}, ..., x^{(m)})$ as input,  \n",
        "$W = (w_0, w_1, ..., w_m)^T$  \n",
        "then $Z = (z^{(1)}, z^{(2)}, ..., z^{(m)}) = W^TX + b$  \n",
        "the output $A = (a^{(1)}, a^{(2)}, ..., a^{(m)})= sigmoid(Z)$  \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-F8Mm4i3zrNc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.5 - Backward Propagation"
      ]
    },
    {
      "metadata": {
        "id": "Fi5_7VE2JMA-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we have the predictions about m images.  \n",
        "But the result could be horrible.  \n",
        "Let's define the loss/difference between **predicted result** and **true result**.   \n",
        "\n",
        "$$\\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$  \n",
        "The loss fuction can be defined freely, as long as it makes sense.  \n",
        "The average loss should be:  \n",
        "$$J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$$"
      ]
    },
    {
      "metadata": {
        "id": "uO-PD_ToLj05",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.5.1 - Gradient Descent\n",
        "Now we have $J(w_0, w_1, ..., w_m, b, x_0, x_1, ...,x_m)$,  \n",
        "the target is to minimize $J$ by changing $w_0, w_1, ..., w_m, b$  \n",
        "Basically, there are two options:\n",
        "* Let each partial derivative to be zero, then solve the equations, get the value of $w_0, w_1, ..., w_m, b$  \n",
        "* Using gradient descent . \n",
        "$$w_0 = w_0 - \\alpha \\frac{\\partial J}{\\partial w_0} $$\n",
        "$$w_1 = w_1 - \\alpha \\frac{\\partial J}{\\partial w_1} $$  \n",
        "$$...$$\n",
        "$$w_m = w_m - \\alpha \\frac{\\partial J}{\\partial w_m} $$  \n",
        "$$ b = b - \\alpha \\frac{\\partial J}{\\partial b} $$\n",
        "  $\\alpha$ is a scalar, usually in $(0, 1)$  called **Learning Rate**.  \n",
        "  It can be proved that for $f = (x_0, x_1, ..., x_n)$ the fastest increasing direction is $(\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}, ..., \\frac{\\partial f}{\\partial x_n})$"
      ]
    },
    {
      "metadata": {
        "id": "DGEfFzaPQJra",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After gradient descent, wo complete forward / backward propagation once, it's call 1 **iteration**.  \n",
        "Then we repeat the iteration untill the loss is acceptable, then the neural network training is finished.  \n",
        "By now the model should have a high accuracy for prediction.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EbAkTeZxa3tT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 2.5.2 - Choose Your Learning Rate Carefully\n",
        "The learning rate should neither be too small nor too big.  \n",
        "If it's too big, you may never converge to minima in gradient descent.    \n",
        "If it's too small, the process to converge to minima would be extremely long.    \n",
        "![](https://drive.google.com/uc?id=1azoR9lY_R2Q-H1V-iKIPq0x1k2d9gW_s)"
      ]
    },
    {
      "metadata": {
        "id": "wA37hNMwRlpX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 - Build Your Own Logistic Regression Model\n",
        "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
        "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python.  \n",
        "\n",
        "The process are as follows:  \n",
        "1. Data Normalization\n",
        "2. Defining Model Structure\n",
        "3. Initializing Parameters\n",
        "4. Loop:\n",
        "  - Forward Propagation: Calculate Current Loss\n",
        "  - Backward Propagation: Calculate Current Gradient\n",
        "  - Update parameters: Gradient Descent"
      ]
    },
    {
      "metadata": {
        "id": "cU7a_mB3aGVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1 - Data Washing"
      ]
    },
    {
      "metadata": {
        "id": "e6VMp117zulq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0r6Z73O-R_a0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    train_dataset = h5py.File('./train_catvnoncat.h5', \"r\")\n",
        "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "    test_dataset = h5py.File('./test_catvnoncat.h5', \"r\")\n",
        "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "    \n",
        "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "    \n",
        "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W9TFQ8u6SCj9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ugmXEW4SJ6V",
        "colab_type": "code",
        "outputId": "abe6c3cf-9c4d-4412-aace-9b8a59d14d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "# Example of a picture\n",
        "index = 25 # Change the index to see different picture\n",
        "plt.imshow(train_set_x_orig[index])\n",
        "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y = [1], it's a 'cat' picture.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztfWmMZNd13ndq7+p9memZ4ZAciqQl\nUZRFybRMSYyhxXLkJdYfR/GSQEkIEDacwEYcWFICBHaQAPYfLz8MB0QkW0hsy/KiSBC8KYxkx7FM\niVrNRSKHw56ZnqV7pveu/VXd/OiaOt851VVTPUs16bof0Oj76t5336373q13zj3nfEdCCIiIiBgt\npA57ABEREcNHXPgRESOIuPAjIkYQceFHRIwg4sKPiBhBxIUfETGCiAs/ImIEcVMLX0TeJyLfFpHT\nIvLhWzWoiIiI2wu5UQceEUkDeAHAewEsA/gygB8PITx364YXERFxO5C5iXPfCuB0COEMAIjIJwC8\nH0DPhZ9KSUil5Lod+98ie6znp1Jp0y6d5nLW1LVazX3LIbTctfRiInas6cxkp9xICtSf//FsUIeJ\nqUmlmlS2dQIdi/1BDq7dYDBn+UmVwXrhcXS9IsK+xa7rpVK9BUvTvxtjmm5oNpfvlOu1qmnHj1Q6\nbR9pPq8wPq3l4rhpV8hru621K6ZuY0OP+dnph67Zld61/PzY+RjoUl0IIVz35t7Mwr8DwHk6Xgbw\nPf1OSKUEExOZTtlCj+t1+42TROtaIdcpFyemTLvZKX3ApmaOmbpyaaNTrpa39Fq1iruWLsZ0Jmfq\nphce7ZQvbrxB+6s0TDskFztFCVdN1Vhhs1OeGLN16VS5U24m3Kf9cUp1LzOC1jWa9EPifpx4/rvu\nBTWtJ/qgc38AwGvA//jxA1wo0Dy6S9VrOt/NxPY/M6P39/idr+mUz738LdOukNJrzc3Mm7pjd31H\np/zat76vU379Wx4x7e49dV+n/Gf/8zdN3af++L91yrulTfRCin5M/UvD/Pi5ukpV73W1QvPR9HPK\nR/uv7UEl+JtZ+ANBRB4H8Phe+XZfLSIiYhDczMK/AOBOOj7Z/swghPAEgCcAIJNJhWuLv+t9L/qL\nmHFvoEAifWipiF1Pxky7XE77OLp4wtQ1wvFO+dLSM51yK7Fva2HRv2Xrtte/Stc6SeOwb5lWS8XI\nELZNXegrqMu+RTjJjQ/9259/8LncdG8C8zZx0itLBwm95ZPE9tFLRN3rQ8uVUO+U02kr9icNvXih\nUDB1d9x5T6e8s60SW2haFYnF9onpWVM3f+xuHS+NsekkFGmxtGElj0ZDVQt/9waVxlkCCF1vwOG/\nEW9mV//LAO4XkXtEJAfgxwB85tYMKyIi4nbiht/4IYRERP4NgL8AkAbwsRDCs7dsZBEREbcNN6Xj\nhxD+FMCf3qKxREREDAm3fXOvG9f0GatlsFrvdfyWqI7fFB1ys2V1wqSp+uLcvN3Vn7/79dS/ambL\nLz1j2lVKuuOfNGqmrtZQPTMjn++UF2a+z7S7uqY6bUi8OY/MhU63szvBPD9W5+yP/fVuv0Pc4h1/\n1wPr7s2E++ht+uyHVp32CZx+my/oPs3d99xv6tLUdmN9Vc/J2GenQCa7TMaacfNjxU45l9HxZpKS\naVfeuNQpX11dNnVNeq66zXSku/PHXc2kZ63Zl/H92174rH37H9QEGF12IyJGEHHhR0SMIIYu6nec\nivpYMFLinE1ERUWBis6tZtm0a7YmOuXxMWvqO0LmvfK9b+qUq7vW3La5stQpV8pOxCYbVaWmvktj\nxS+aZgszaoaq1qx3YQZ63G3VSe1b5x0D2VTmfbRY/GaRveVEfTbTdTmKcP99POv6QXqIwJmsfeRO\n3KVzdfToHabu5Zf+Xg/ItFp093aiqB6Vc0fvNHUJOQVdWHqhU77/XqtW7FxR8X5l5bypawXy9ERv\n8HdOuZtr1bo+D3/oLc7zcbdPzMHc/OIbPyJiBBEXfkTECCIu/IiIEcRQdfwQgFbbNZJddAEgBdYJ\nnemJItyMyyu5ggJAra59thq2bm5KI7Nap1S/q+6s22vV1cyTcr6seQq0kLrqnKWtF0y72Xmtu/fu\n15m6clV1/K31XVMXjN6tfbSC32tgvdsHznAfVG55c56Wm84Vl1V5kd5GKtYzvYbJ+i676S4cWTTt\n7r1bg29WVqwZbXtL702B9gaKORs8VSiqyW7uiN0nqFJQ18SEuvZePv1N0+7Fjcud8sWLZ00d75V4\nE2wveNdsO1f+fdtPr789iG/8iIgRRFz4EREjiOGK+ghI2qYjJ+kbAaflPNWSHiYlSVuvuApFepXK\n1ky3MK2x3cUxFbc3rljRcGfr3k45n7eegeVN9R7LUBz/5as2rn5jbalTnpm2pqejixrVl0ktmLpG\nTT3Qmhv6XdhzDHBmOifCs/kqafY2xRkBvo9piMkwxHlUpriuhycZAMzMznXKr7n3tbYP6nN7Y9XU\nsRktl1WRXRzZBrI6x6ls3lTNjqup7/jCjPZRtirehQ19XkrOxNuHQ6Mn+nnuSeitBjC61adBrjiY\nqhDf+BERI4i48CMiRhDD9dwL6iWWTnvRk+m17GlMoMA7/Cm3q99MVPxuOG+0XFpF4GpLiRWS8oZp\nt3BCiRtWm5aWq1bZ6ZTHSPScKNmAj/VdPb54ccnUseg8OTlt66bU8zAJTJu1ZtpVKmoNaDpSCvbC\n66cSsNiYy9vAFpYoc3n9nvm8FaOLRICRNOw4Mjnt8/hxVW8mJy1d2uULS51ycHx2abIotEiF8Rx+\nE5NKvtHy3oX0TNRL2sfxY6dMs3JDl0LyV39m6tjKJAPK392sdz1IVrrA2/+91TNPl3ZQ0tz4xo+I\nGEHEhR8RMYKICz8iYgQxZHOe6pqt3s5o+5zYg0EyWH71pK76+qVLNsLqysWXO+W5BTUvTU9O2nZL\nS52y14tzY9q2sqMmvCmnt5aJ971RsxGEq5fPdcoid5m6I8f0eGFRzYxerxwjconNDcsBz1z9NRqH\njxZrtlhndv0T6SWb0cbHJ0y7xWMa8Vh31NjjRH2eonu2etnel3pV9ytawecZ0DFnaJCFMTuOhWO6\nh1Ct2H2ZjVWd7/QJJVx98M3vNO2e+5amg0gSu3dkB+Wj7m4f/PbBjfLs74f4xo+IGEHEhR8RMYIY\nujmvY6XqJ7d4cYp+nthbLJX2Jg3lyDu3vGTqzr2kotzMuGbBmZy13nPJi893yuNTR03d7MyRTvnK\nMnHR1az4yl6CKxvWC6xeV9F/a8tl0iHuuOK49nGERFkAqFa0j7pTJSok6tbrKrLmcvZWV2ta581j\nbCnKkqg/OTVj2i0saMBNKm/Vna0tVbuWiFDDe8XdfUK/286ONa1m+f6St15qzF6rzsQhLcuTeOyE\nqk/veO8/0evebdWs3/8fqgo2+on6ni9vQCud4dzrevZ7p0uzffS5QPTci4iIuB7iwo+IGEHEhR8R\nMYI4BF79PR2kOxV2n0yjGSKsJI59uDTZrN5s71hd8tlvKX/+4qKa80obVq/MEPlGrWqJMmaOqzvv\nUTI97ayvmHa5tJJoTE8UTd1uVfXHhkv3vLutnP5sirvjzvtMu/GiuvpWtm2UWdLQTL1p0pG9S2ch\nz2QWdr5zOTXnZWnu5+aOmHZ33qW5ChLHZ18q69yV2KXZe9SSWbHlohBz1Gc6pY9qJmPvO7s+txKr\n4z/wxrd0yo++/bs65W9+5Sum3blzL+kQnanZPo/O9bmHZu+fYeuxO9j79gYzmw+E645ARD4mIqsi\n8gx9NicinxORF9v/Z/v1ERER8crCID89vwPgfe6zDwN4MoRwP4An28cRERGvElxX1A8h/LWInHIf\nvx/AO9vljwP4AoAPDXLBa+JLy6d37sMZLmRuCn24+fgwaVqTzPMvqDnvOx98Y6d86WWbQksaKqaP\n5W366zSZtvKTatrKTlgzV+mqirbeuMJ9pBwbyfb2GrVTcfvICTshR8mMVi3vmLrNTe2jUNA5KOQs\nIcjOrp7nOf2MKE1fIONILubJ07BUtRGKzbqaFbPEkdd0acmqlKa84TwlG6T+JZxbwY1jivgUQ8t6\n/83N6z3cvLrZKf/tkzbl45UrmkKrm0Wj54Ft1kcW59TY3erBwWX47msNJzpvMYRwbaYuA1js1zgi\nIuKVhZve3AshBJGuKOUORORxAI/f7HUiIiJuHW504a+IyPEQwiUROQ5gtVfDEMITAJ4AABEJodeu\nPpVTXYIIkVIwQUXT/96wOGXrNjZ093u3rCLw3a97xLS7cvFMp1xzFN1NIu1ISCydmjtu2m2ta+BM\nq2l37vNZ2uV3HnOlbRVFJ2dUnJ0iWmgAGCM66aOLljOwXlbLwLlz+l08f2CtquNqBrubDjqen1Pv\nxZN3vsY2IwvI5rp9BGrESciEHVsbdj7W19V7seVE3jR562UpMGdi0qpWy+coAGvOqme7dC/++gt/\nqedctpYYDsi6HbvpEoj6vY9UPni23IOc140bFfU/A+CD7fIHAXz6BvuJiIg4BAxizvt9AF8E8FoR\nWRaRxwD8MoD3isiLAL6vfRwREfEqwSC7+j/eo+o9t3gsERERQ8IheO71iCLitE1ODhFWuog7P9XH\n7Of1sgaZkc5f0Gi6hx5+1LSbWlByiRef/6qp21hTvTAzpnrmsZNW971KaZYrVRs9l1C65xRsKigW\nwBKKrFu9bFM6LR7TVNDjjgSE67Z2dM+gmTivuLy/tmJ2WvXkeUp5NTNr/bQ40nB56VumboN0/kpZ\nTX1dZii6tePjlny0SLr83IxGUS4eP2XaSdA5PXnU9jFOuRe+8MW/6ZQXTr3RtBsb/9tOebe0aeqC\n2TsaEF02aSaQ8VGlPTux7fqkLDsooq9+RMQIIi78iIgRxPBF/R4ySugj6zPhRqaPvMOnpT25BB2/\n8KISQ1w+97Bp99Z3/UinXJiyYuOX/q+ag1rEZ1923nNTRO5Rr1tPNfaY89zrTHSRIVKOctmqCzUy\n2RUL1iMvmdVAmrGimsAaTtSfZm89sY/B2LSK9Mfu0JRiBce59/JL3+6UVx3HIRNutMibLuOCefJF\nVVWmZqwpLkP3vTiuJsGUWE/DYkHn6viiJU9ZXFRT6513a/qus1es+bHBptsungzZr9gFU9Ulv/cx\nxRkeSVPTcxx9NKaBEN/4EREjiLjwIyJGEHHhR0SMIIau46tu0ltJCY50n/UZjhzrSjdMeny3jq9t\nNzc0gu00kWsCwLv+8Q91yve+5h5Td+6FY53y2XNLnTITVwBAcUJ15ImG076yqp8ndcsB32zq985k\ntM+Cy1m3dlXNivUJq3dPz6qefNfdSpSxuW759xt0Xmja+Z47ot9zdkF15FzRusqWS7r30HB9sN6a\nIcKUdNqaEadoX0NcHsCNTTWrpYiAZXp6y7RbuFsJUmbmrQvz1KKSmBw/qeQg/++Lv23a1cjs2k0S\no+U+/BoDfQ4Mro/7cXjz9Q11eq2vgzWPiIj4h4C48CMiRhDDN+d1xBUrmzQ5RXJXdFQvPr7eMlmr\n5UVPPY/TTJ89f8a0+/pTX+iUZxZt1N14VvvPGd43O44p8nwrOG+0TeKbb9Usp1+aTF3b25SSe8zy\n9uUprbUXB8fG1Oz1ugff3Ck/+w3LMVfe0fmZmLG5BeZIvE8XtL9S1ZomWxQ1mR2zEYQZMnFypJ7P\nA5Ai8hFxakC1oh5/pZKaB7dLdt5KFR3XpcuXTF0mp+O6QPz+5W2b06Dm+A8ZhvW+T1TpoFF8nv+w\nZ4prrw2zJdtFwvczM+6H+MaPiBhBxIUfETGCGP6ufvu/pzDmIBIvtvQW9X27g44CSKXsFLx87kKn\nPHbhnKnLtlRMzVNW2qrb0U5nVDSfdDvyE5RFdnPLUmOn0yrCT8zqzvq2o+8ujKvVIJN14nFdA1aO\nLmq7qUmrLtTK+r0XXPBNtqB9BrI81B2vnpB6VshajzwOc2G1y6tgJRLbMxn7XZh0pUQBR1vEKwgA\ntYZaX46cOGXqmlW1ADDJysqVy6Zdg7j/uhjx+m3r83jNSc7iZDj3BtuC70XdvXcxn7U3dI+hD+Ib\nPyJiBBEXfkTECCIu/IiIEcTQdfxr6lJwjIPMm+nUf0iKOfcHVeS9yYTKdAGncmJ8WqPbdldeNHX3\n3KFmrpBTz7fnzliijFJJI70yGTvFkxSBNjFldesamcsaVebmt9+ZiT5nXOrqXdo3WN9Ub715IugA\ngG1K1zUxa82WHPHHfPNVZ/Ja39RrlXZthGKtqnsDSaL7IU2XJivVR//P0f4F91Fx5ryEPP5S7r6/\nfFoJQr5JadS2S3a/guHn29R1BcztT9JxY4mw/Tk+XVe/xsPh1Y+IiHgVIy78iIgRxPBFfdmfV59N\nF60+7lE3SnEejFeffn7+4rJp9/Jp9e46Pm892r7jLe/qlNeIp39lp2HaXXzuWT1wwULbJIpPzVgx\nvdnStjtb6lm2tWG9zCaIIOTYHSdNXaDf8l3q49S9rzXtODPtHffYbLwV4vfPbOgjcunMS6bdNon6\nO9s2cCZJ7Jx0xufuLYv3Y0VLKpKnVFmc5qvVtH0H4txbW7NzdXlNPSUrNVUJ8gUbWFWtUJBOy+UZ\nGNwlT09xVX1Nc9yFOcf10SejdFc6uesgvvEjIkYQceFHRIwg4sKPiBhBHILLrlwrDA7DRXgjpj0/\nCO1jZdW6bq6uXOyUmy6S7Nx5NdsxgaTfkxBiTKhUrJmrzuarhiXiyBTUrZb3AhqOsPPC0gud8vj4\npO2DCCo3V1VXv+87HjTtpueULz/jDKjbG0pEuXJpqVO+4gk1d1R/9nox35l0WiMZvTmPCVNyzv2Y\nCTCZbJNNewCwtqZmyxdeOm3qLq6SW3RW9xAkZe9LMD7kvU3B/dNT986/x3sqLefi3WsPq5/WflCd\n3mOQFFp3isjnReQ5EXlWRH62/fmciHxORF5s/5+9Xl8RERGvDAwi6icAfj6E8ACARwD8jIg8AODD\nAJ4MIdwP4Mn2cURExKsAg+TOuwTgUru8IyLPA7gDwPsBvLPd7OMAvgDgQ9fr75qo5EUmI+IM6IXk\n2w0q+nOrXeJ/B4DTZLIqL9j0VF/6/Kc65Qff9D2d8rHj1qT20pKmba64/tMUxdYl8VF6MP4uiROj\n6yRiP/eNvzN1J06oh16Zrn3+rCUcmSKyjUrDis6liqogG2sq9u/u2NRSNUrz1WV6ovGzOD/lUlyn\nybMx7bwcA6XrTlHK7Fzemv1KO2pKvHrVcgsmLR3H5Ix6ZVbq1iTI6dG8517f58pw7rPJ2N2zROcq\naXiTZq/nffC02Eo0M9jaOdDmnoicAvBmAE8BWGz/KADAZQCLPU6LiIh4hWHgzT0RmQDwxwB+LoSw\nzb+CIYQgIvv+1IjI4wAev9mBRkRE3DoM9MYXkSz2Fv3vhhD+pP3xiogcb9cfB7C637khhCdCCA+H\nEB7erz4iImL4uO4bX/Ze7R8F8HwI4Vep6jMAPgjgl9v/P33dqwl5P3b7I1LRpxG+2aTAveHNUJcv\nX6Q6a0abINPZLJn9koI1aOTy6g5anLDmtjHqY/3KRVM3QbnuZheUgafp3F8lp/sE5bKNMtsgth6O\npnv2G0+Zdm/6rrd3yq3JOVPXIDJSJsqsNuw4OCrOhznmiSA0Q+bNaeemXJxUt+im64PzB3IOQq/j\nQ/QeNhIbQcj3N0VmRe9SLJR4sYsMk8109so9U90lTm8PZIJMkt45CNgULF2knL0OgIM6sw8i6r8D\nwL8A8Pci8vX2Z/8Bewv+kyLyGICzAD5woCtHREQcGgbZ1f8b9P45ec+tHU5ERMQwMFTPPUFv04jZ\nLLxJr6SbAXvhJUibuvWyiofLV9Xza2rOprGan1ezkSeQbJEourhgUzrvllWsLpCoPF60RJlZ8nBr\nOtZSjuRjs9T58y+bdseOa6qpe19vxe8tirrjdGMN5zHHZiif9ixLYnqKIyNdmqwmHR+7425TNzal\npCUJqS25vH1sVy5phGWzWTd1GVIL2INw3OUByFIatFq1H0lH72O+FU1nq22ZOmfOM557A/rudQWw\nxui8iIiI6yAu/IiIEcQhZMu9RrrX23PPawO3cVN/H5FJkcna3eNsXsXDjR0VBwvTVnzdJU8yOKtB\ncVJJNEqb1sts8ZiK30wMkXZpUqtl9cgruMCWMnnTNSgwxPPlLb2swSxTc9b3igOVSmUdR9OJ6XUK\nosm6MfIudiavY6w4rrtqXWXgXM4+jkczqi4sLJ7Qa6WtCpbPqirEBCYAAB4H5S3w2ZT7ct33Y8cw\nXI5h3zLgn+E+Dx1fqt868MknIudeRETE9RAXfkTECCIu/IiIEcShEXH0DEjaa3RD6Ofh17vOXoxN\nPgvz1txWpMiyhKKtJsatOW93i0gim5ZsgxW6ucVTpmZsQvtprWgOv6RhPQgb5E2XStucdeyRVq3p\nGFMu8m1tXXXhZ7/5tKnb3NE9BM4p5z3OODrDv0HY44/LaafQtuq6D7Gza/X/adpDKNNeg8/hlyFS\nznLZzvf4pHpK5rI6B55sM0fmx1rVErCYZ8Q9uC3LEkNl91zRBKXc/kKD9oG6iGZvE+IbPyJiBBEX\nfkTECGLoov7NYsCMxYPDW0Wo7NNfpZhogUS5zW1LUCFEGjF/7ISp26DAnPnFu0xdhuTBUk3F+2rd\neqPtkChecAErNn+AiuachhywZrqNTZuu23joiao+4kTU0OTQbKsGJCSms/iay1hTXAt6rW4hV/us\nkzmy5Dj8x8aVMGVq7oipa1Ka73Ra1ZZszn4Xc6/dg2VINbrIX6iK1ICUe7A4HXvX96QgI57GbvX0\nVjzw7fHcsp4iIiJeNYgLPyJiBBEXfkTECGL4Ov5Aakpvv8gb1+t7RD2J18X0eGPtkqnb3lIX2xMn\nNd9csWgjvVh/3nFkmxxZl05ZffcK6f8NMuGl4Igb66rvdpFGsJJIexIp9z2Z2MJz3bOiaaImXQQe\nu/A2gn2HFHLkSkwRc95VNpNXM9rMrCU0ydJ+wAzlMZybt3r81rqSPzWc6TNDfczNEpHKuN0nWDrD\nnPjOZNfsXZdK7W/O8+ZqJtUY1JLtST9vJSFNfONHRIwg4sKPiBhBHII575q44sWYwQgIDD/ZgeT+\n/fvMZGwfeRJRM870xOIxm95qFevpZX5NHenCPPHZT0xa3v7l5aVOef0qia81K76m6XtXatbUZ8xB\nHC0WHAEGicSJ49LLEPd/mubAzzeL7d7jrEnXFpqRhjMrFsf1WqUdm9Zql8ykm5c0fdkDb367aZcv\n6D3LifXIS5E5srKjKtLKFaci8bw5zaeZsGo4WBRfl7rgvnevaxvcOutdF+IbPyJiBBEXfkTECOIQ\niDh6yC+h987pQOffIrAI73fdxyiwg73iqi6oo5mot9jWmiWGKFIf6bz1+EuTOM4aSCBCCgDIsodf\n1RJssHcdz1XK/8RTXTZnyTzYSsGZesulXdOOd/XrVauOBHBgjs5VMe+DivS85QsvmLoMybr33qu7\n+ifmLB34GnnyrW1ZK0qppmNuNvV+bm7a8VarOl6f/sqoTO7xM557/Ay3vPcfWUpsF4NzaAyaSncA\nxDd+RMQIIi78iIgRRFz4EREjiEMw5+2fzteTE94s+no5kWImTuPiKK1y2eq0TNLBp1VL1gy1s6u6\ne8l57gkReBYct/sYebEldfb+s15maRMl6MxonP6ZdFXp+o3X8/KOsHOcOP2r5IXodd+ETFRdfPNN\nHUcup/OWdubTFJkOA6z+f+KI3ouffkxTL66kbFry9EUlMCnVbf8lIuYwKdFb1ryZMXPgohDRy97m\nzID0UHQ/zpwiztYMnt791uWeuO4bX0QKIvIlEfmGiDwrIr/U/vweEXlKRE6LyB+ISO56fUVERLwy\nMIioXwPw7hDCmwA8BOB9IvIIgF8B8GshhPsAbAB47PYNMyIi4lZikNx5AcA1mTfb/gsA3g3gJ9qf\nfxzALwL4rQH62/vvudfYI6/HOd21g4s7bIYRU7ZXa5AXW9L0oq3WlXZUnPf8bbNzanq6++77TJ2k\nVMQ8+8LXTd3ahgYBtci0Nz5uyTbqO2qKEp+SqqFj5vG3HKlINq9zV8jb8U/PqLmMVZXg5oPTRKXd\nK2Q8r+I939uKC/QpkKddyo3j5F0aVHPyTW/slC8+a4XLOo1LMlZd4NRYnMeq7LwtOfOvJxVheDG9\n16N5kNwQ5vnuJ/VTQJA4c+FBA3gG2twTkXQ7U+4qgM8BeAnAZlA/0GUAd/Q6PyIi4pWFgRZ+CKEZ\nQngIwEkAbwXwukEvICKPi8jTIvL0kAhEIyIiroMDmfNCCJsAPg/gbQBmROSa/HgSwIUe5zwRQng4\nhPDwbXa6i4iIGBDX1fFF5AiARghhU0TGALwXext7nwfwowA+AeCDAD59c0Nhl91+A+pX1buy14+O\nJ5BMp7Uhc+cDwMysEkDkCqp35wtWB2dSzoVJq7dWr2qUWdj4kql74ayaD9eqOo5i3uq0nGJuatzq\ntGXy4C2TV6rfr2BX3FbLfs8Cp7jmCDynnwcKY5so2EdpklJZl2nfQcYt2UbIqEkzlbHz+IY3nuqU\nN3Y1knFj0/Lqb22rObVSsubTRk11eSYRLRRcmuz0/hGJgEt5PWjmam+yu5G9Ke8eTBtV3nW403hA\nsXoQO/5xAB8XkTT2JIRPhhA+KyLPAfiEiPwXAF8D8NGBrhgREXHoGGRX/5sA3rzP52ewp+9HRES8\nyjB0z71rZoduSYW96QaEWDGdeeW6RKGeJhNPIKFiXcqJxzOzmlLrrlO6v5kfc+YlilTLOxPVax7U\n8+57u/X4+/ZHNTptlSLJdhNrskNNTYlTzhMuO6a3lGdnq2K/C3vhcWQaAAjxDjbJhOm9BDl994RL\ncT1Bc/J9j6qn3T3f+bBp97E/W+6Ui85s+Y/e/mCnfPaimvZSORut2CKyjVrdkookNHesqZQrVl3g\n52CsaO9Zi56DZuLVHS0PyNHRZULudVqXr2W/C7TrPIlIL0Rf/YiIEURc+BERI4hDTKElPY/8Djwf\npynjacq1S1EQTcp5cLVI5GMCiVTadlIlj65Wzop8HDiTkHfXRN7uEOeyGuRSnLKkEUlKz1t47UOm\n7r7X6+766b9b04qUFV/H84sONL/xAAAfZUlEQVSd8nRl1dTVaAc6U1SRuNqw6oKQgJlxhCOB1IAs\nBdFknAUkoRvj+Qlfc5f6c/3UT393p3zqTe8z7U69/jQNyoqy2fy9nfLqus7NuiMEuUr8hLWaFeFT\n5FKYp2cn6wKTAo0/Kbm5Yo8896o0Tn79JPHeVQPv9xvLQJfYL+3zB9vVj2/8iIgRRFz4EREjiLjw\nIyJGEIen43v9nD7wenc2p3pmcVz16UzWmnXYCy+ft3XsqcY8+A3PKU+6nifbrBCxZY0INqvVommX\ny6lZqly3OleGlMQXL0ybuh/+/rd0yk+9+BXt3/Hq/9Q/faRTPt46b+p+7/e/1ilf3tXvlkk7PZ7m\nO3FpuNlExfcl6/R4pvQPKbunMnPknk75i1/W89ZaF0277/ruH+iUz563XnfPPrPUKV++qibMs8vn\nTLvSFhGflCxpSYbMjMYc6UhF6hUyabroPPZeTBoucu+G4k/6pIi7ke5u4Lz4xo+IGEHEhR8RMYI4\nhGy5e0JJytni2JspX7Bi+ti4em0Vx7RcKE7AgrPD2pommeY4IKNcsaYhm/HUinVVkm0bZTUb5Sgt\nFgDMz+q4Vi4tm7oSEVRUStZc+Og71FPt5/+1zsHaphWB//k/Uw/q1dNfNXV3fFG9AbfPkldcyao0\npaoeV514vHZFswRXqzo/4nkSOatuzqo7UpzvlFfKKvaf/by91l1n/r5TzhdnTN1Ly5c75eVlVWl2\ntjdMu0KWTXbWTFcmzsBA+Q5Kjicx4QzEsEgoyOhWhJZLD1PcXv+DXcAHpN0WIo6IiIh/WIgLPyJi\nBBEXfkTECGK4Or5Ih5ve52tLZyjVsasbK6jOzK64rKsDQJYIJZvOXJPi37hCiysMmmTe2921emCT\nCDBFiJzRcbRvb6tePD5hTXaVkrriZipWT/va86qPPvjAd3XKb3vE7mV843nNx/d3f2VvYXpW3Vxn\niE/y0lWbp69JEWc1F6l2cVnJQloJpwa35jzOM1B3LsE7RNLZEh0jc+wDwLmlJe1/3Lo3l2t6LwrE\n9e/12VpJv1vBk5ZQJN/KJb2fpVLvnAk+wu+WpHyQHmXghkyCB9XpPeIbPyJiBBEXfkTECGKoor6I\ndFJUFcas+Jp1hBUG5DkViE0hOA44mLTQjjeNRHMW3Tz3WsgRF32w4mtSVZF4ZVVNTZmc/S7zR9S8\nl8tbcolmi1JcW4c5LC8rX2mVUj89PzFpG5IZLQlTpmp6gX7Lc1p+7vlnTTsmLWk5WbNGpi1OG15w\nKliZCEd2XaqwCxeWOuWFO9Scl0rbPr761F93yvVg30MFUpPmiOt/enbetJssHuuUK8402aQ0X6m0\nPu4+mrBFUYg+PDSp7//seNys+D1MxDd+RMQIIi78iIgRxNBF/WybijrjiDJSFLzSdFx37C7FxBAt\n2HY1olLOZq3qwAE8TNgxPmFFZQ7maTZtcAw78uXIQ0xcNtV18nzLukCiFFkeJGvnYHtdd/yXzr2s\nY6pY68LctHq45TP2t7teVq+240dVPG45GZX58ryXY57GVSDq8NCw81HMa7tKzeotF86d0f5TOge5\nglWL1jfUQlFzloEiBUItzCm1+ZhLWcbEITub1qtvZ0fnbpPSnjUdj2E+3zvnq/Uy9Vme9z+nS+o3\nx7c+wYR6vkYijoiIiB6ICz8iYgQRF35ExAhiuDp+KkXmLcdn3yT+9qZVnJKmDrNA6lHTpYjmaLGs\n417nvYEs6cVM7AEAddLBV1ZsZF1COm7+kkaL+TTZ0zNqXhLn7cZttzetN936uuq7G1uqq1Z3XfTc\nmu4FFNN2ro5O6vWuBiIfSex+CHvhFYt2DiYpzTdHgVXLdq+BU3s57hTUSYdeuaSegNNHbFLlhUXl\n3OfvD9i05BvrK51y3uUSKO/SPDpi+ZkF/S7bZZ3HbN7urwiZbms+zwBdzkeVMte9sS57vo7A5Zs3\n+3lu/oNi4Dd+O1X210Tks+3je0TkKRE5LSJ/ICK9d0ciIiJeUTiIqP+zAJ6n418B8GshhPsAbAB4\n7FYOLCIi4vZhIFFfRE4C+CEA/xXAv5M9OePdAH6i3eTjAH4RwG9dv7c9MSdp9TbZccAOYAMoWEhq\nNqwJKUsmwpQzmXAfzNWXzViVoElmL+nim9c6NjkyFz8ATE+oiF133n9jlEXWi43nz367U15bU7F3\nbv6oadds6ZjrzqS5uqXX3lxST0Dx6a9I9SlO2Ay2M9N6nJAK1mxaj7lqTT38pmasd2GWSFLKFBAz\nPWMDcWaO3kVjsl6OgM7d5KR68dXcfc+R9+Xqqs3W3iQijl0i3/AmTL63XhRn4gwvYbda7C1K/TlV\nlq2pvUyAw8Sgb/xfB/ALUM1lHsBmCJ2nehnAHfudGBER8crDdRe+iPwwgNUQwleu17bH+Y+LyNMi\n8rTftIuIiDgcDCLqvwPAj4jIDwIoAJgC8BsAZkQk037rnwRwYb+TQwhPAHgCALL53KsniiEi4h8w\nrrvwQwgfAfARABCRdwL49yGEnxSRPwTwowA+AeCDAD593auFgOQa0UWfnwBnAettuvD6FpU9EQep\ni8iQu61P/cxdpt1AUimdrjyRdzYc732ZovimJi2BZIUi2lYu29/KOvPbG1OlHeMdd2q02/y81bvL\n20T0UdAIws0dS7bBPc4uHDN1U1OqT7MLc7FgCTXXrmjOuu2S3ed44+s0R0BpR81oC0dPmHaTFGnn\n3YpLRITCexJ8HwBg/Sp9zzWXS5DMmJvkEp0Vn05b577loz77wOj1gSMe7cOZou/W8vsLhxDVdzMO\nPB/C3kbfaezp/B+9NUOKiIi43TiQA08I4QsAvtAunwHw1ls/pIiIiNuNoXruhRDUDOakG05X5UWt\nOpmN0v28qAJ7UVkzGp/HvHotFyWYI0IQ5vMHgEZtf+71Ws16eq1fVS+zhktPNTOrnmTNuhWPmyRW\ncxrnCWduy5DZyxNDZPM65ukFndPSM18z7caJw27SqSMLi3d2yvmCjuPSeZu6ij0n6y4VWUImt6kZ\n/c5w9zZNsvK8y0/QID77KuUxCM5EeuaMmkF3d603JKtPLZrvQtY+OzVKjdVtzuvtJcdtrYefa2dy\nbfeO8DuAlnFTiL76EREjiLjwIyJGEENPoXWNtKLbc0+Pxf0cJSTaNUhm93TPhvI6a78a03KnSdQK\nbhwh6HmepKNCnl8mg61PB0Y70DsusCVbVFE84/j4isQxN0/eehPT1tuNzR4t/9tN37NMRBZeWK2U\n1ZvuylVrXbjrHrUaTEypmtFIXjLtdksqfs/MWHVhfka/S66ofWQc+UiWiD7K23au2PttdVWz7G5t\nrJl2q5e1Lp11FOA0PfxI+CzJLGKLn1OavMHVgMHVBZbuDeXHLeH13h/xjR8RMYKICz8iYgQRF35E\nxAhi+Dp+W0fyunWDzG+ebDOX12g05svPuBRavDeQdvYUwx1P/Oo+FXaTTHZeM+a0XxX2zpt1Ojj9\nnpZdqqZ6bUmv7Ww3eTIlFsZ0f2Fq1kbnZSktdHC6Y4rmhAlBFxetd96FZTXNlbeszlwljvyEdOGi\n05+np3SMmZwlI6lVyfSZ0jlImrbdDrXbWLti6tavqFmUyTa2ty1hR4v2h1LOLS5HYw6U0yDxW0xU\n9ntMDK+rs87f3wGvX6XZRNi3765jf98PSMwR3/gRESOIuPAjIkYQwxX1Q0DS5mJrOVkrGLHaisAs\n3ufJ/JNzhB0s7XgyDxbvmUfOC2DM85ZznPhT84udcmVXxddyyQbA1Gt63HCZV9mla3raBtiwOa9O\nnm8pl+6Jg0FS7re7RubDEvHIH120dAkbxG931933mrppQ8Sh9+I197/OtLu8qvkDzl2wXn11SsNV\npRRUVeflmNAN8EE6u9vKO8iifnHMcgSyubNesapVVvQ5290hj7+Uy6bcJ5stm9W8asgPUOhR3js2\nyoSp42CwJj1/XU58oZ9J8GCmv/jGj4gYQcSFHxExgogLPyJiBHFo0XneZMfElj7FdY5MRRzFJ85V\nlvOhpV30VYGJOOm8xHHzJxQhV/B59RqqR7GuWnUmO8O/79J/J7S3Udm1+u7ElOrnp+5/sFMubduI\nszwRYlS2nU7LLs2kJbaceXNyQskwH3jDQ6buPtLlr6ypnr14xO5JbFCeuhWKSASAWTJxnj+71Clf\nvmj3AtiF2c/32JjOXWmX3KB3bJ4B5sgXx6u/Q2bXGrlZJ4l//jiXoJ0roWNfx0QuTP4SumjmtP+0\ny3fY4DTcfUg/jRrvVfoDvsLjGz8iYgQRF35ExAhi6J5717zVvBiTItklle4t6jNBhefEY7Hdp0FO\nUmomYZ50P440pdBquT5qbLZj0TBnpzGX4ug/2wfnAqhVLUmHEEf+6rKmmW5UrOfeFKXJrmyv22tT\nCNqReRW3PWnJ1oaa7LpINBIVUzki8bwzlW1uqQrCJkAAmJxQEZ7noNGomnbplqot4kxlubTOY62i\n3oTMaQgAY6Im3rSTgUslvV7CabidpxtzYzThU7Pxgakyzws/p/75a9C16zVb582Yva7VDwfl6o9v\n/IiIEURc+BERI4jh7upDxaaU22VO0657zu2Ep0l8bVJgS85lxC3Qec2ujLvkEUWiVtYFl7Don07b\n6Rmj61UrWpcem7DtKDDk6poLKKFyJmeDjDgl1caaUka72BgUaBd7a9MG2ISErBKkgpw4YT33OEjq\nueeeMXWbZCloEJlHyakmZ8682CnzDjwA1Gn+t8nLMTNmeQxnF1SNEWfp2dpSNSNDz8fEhPXcY7G6\n5NKZ9dox7/LOGzTZixO/k+b+qlt3MA+XXSep/WV66fL+2z+Y50YQ3/gRESOIuPAjIkYQceFHRIwg\nhm7OuwavAwmb87znFHnrsZrjvf+yZFqB416vMcEG2T4yjlef9UVPyMieX+ytV3VpmzGpOv8UpXcG\ngEaB9iEadvycVNTmGbDt+Nqe0CRNRBwmHsyRlkxMqUnwa9+wnPvrm+oZl6M53diyHnMXLylJ5yNv\ne4ep26K2nNY6caYrvreVijX1be2oCS9NezFTRavjs8mx1fRzquUacez7PaAbxqC6NtPqu/0F6cH8\n0U22OSjpx/Ux0MIXkSUAOwCaAJIQwsMiMgfgDwCcArAE4AMhhI1efURERLxycBBR/10hhIdCCA+3\njz8M4MkQwv0AnmwfR0REvApwM6L++wG8s13+OPZy6n2o3wkCFXO8SYP553z6K+bcz2VVzGslVsSu\nkcdZw4l8FSKoyBOfvecqM6NynHiFgprziPYO1abNlru9rdfOe5PjmF47m7ZkIdvbKh5nyItPGjaY\nJ9VSkbjh1IwWic4bG+pZNz65bdrdd9/rO+WLF5dN3RqRdKTIpsReagDw+je8oVOecxl3d0kdWTim\nKbm2tqxQuEumvokJa+rL0LV3tnX8lZr3eCQVyY0xP0Z8jeTpWXepzRJS8fplr+3mx9+fHKMfj/6g\n8EFohgTE2/raVYOqAIO+8QOAvxSRr4jI4+3PFkMI1yhYLgNY3P/UiIiIVxoGfeM/GkK4ICJHAXxO\nRL7FlSGEIOJ/gvbQ/qF4fK98U2ONiIi4RRjojR9CuND+vwrgU9hLj70iIscBoP1/tce5T4QQHm5v\nCN6aUUdERNwUrvvGF5FxAKkQwk67/P0A/jOAzwD4IIBfbv//9EBXbC9+T7aRJ5fPfMHqxax5N4iM\nEE0XVUZ6W5cpjvjbi+Ri612H2TxWr1vdnX8mjRnQmYbKJdVHd90+wcSkkk1knS8uX3unqtduuGg0\nJgQdcym00xS9WKHxb2xZHX+GCCofffQ9pu7ckubIO31Gy9ms3TcpUhrxmp9vo0PrD342Y+/t0eMn\nO2WfO6/e1Alv0uT7fAQc8Sf1fqnTOaW1ve85cp/2/feMnnOwQq9/ybEpbtAXoDN99thPAIBwze23\n2bOJwSCi/iKAT7Xf1hkAvxdC+HMR+TKAT4rIYwDOAvjAYJeMiIg4bFx34YcQzgB40z6frwF4T/cZ\nERERr3QMP4XWNVHfibljRRW/xxxvupA43iQTXiux4iWbaJrO1JfJMFeffu493ziKL+ei5zgaMGM4\n/KzYyKQXzcSKjRWKHstkLcdcgb53i/oPTqXZKmkfPjKQcwFMUYqrskvXvbyinPjjLrKuQmpRk0yJ\nItb8uE6ce5l80dTt7Or1VlY10hBuvjlt9rbzDFynyMMsRUpmncmukNc58KQi1mzHJmPHuUdStCct\nYbNalwDPKa84X0Mfvrx+5kJzXXe1QbbIwoDsHdFXPyJiBBEXfkTECCIu/IiIEcRwdXyRDkllxhFl\nMgOPOFPfGLnYGs76MatXZkjXDi2rj3IuusQQIVqdiFNLhy4dq1d+P0ccSs1yebtPkKXvWSvbaLTd\nRN1X0yk2gZlmyDPPe8bq58zzznnvvA8Fz+Omy2e3SumqeQ5KZUu2OU5Rcbtly3xTLqsJskrmyCNH\nT5h2nCMg7Nj+j53UnH5XiI8/uKxyu2UdfzplJ4v3X6yZzuvCQu2cGY3mzufO62Wa686dx+WDR/T5\ncfQ+J+r4ERERPRAXfkTECGLo5rxropH33GNZqOnSGxk+dELLiVlMnJk4U1+rpsdM4CFO5SgU1KS2\n41JjIadqwASle9p2Ka6YzKPZ8uY8jqyz34slOfYsq7sxZsl8lXjzFXkNssffxIzl5i9dUQ9rTj0O\nWNG5Rmm+PXd7q6VjXCXzIACsXdHjJn3PC8tLpl2tqnWeOHRiUslCJsnTcHfbEphm6X5Wq1ZtYXmZ\nxf7QJbL3I7lgjz8XMdcj5ZUX5wc24fUR57nuZt3f4xs/ImIEERd+RMQIYuii/jWvKE6FBdh0WF6K\nYb68LG1xO2XBiGhePeA+W5Rqy6fJSpGHWHDef5xJl0WtjPMkYzGyUbeiOGdUzbisqcwDx6JhsWi9\n8/LG282m0BojPro0EZisrV407XiHPp+3npJp8nLk+eZAJwBYuax9etWqXqfUVTTHBceXV6c+ExcU\ntbaq6gJbSlqt3mJuV1o1unaSEK+j87bkHf9+YnqXiE1NW/1ybRmvPlfFH/CufMo36z2Oa8eDKgDx\njR8RMYKICz8iYgQRF35ExAhiqDp+SlIotvPbeRJK9lTr1r/YvEcmDZ+CjOp8/xVK8cxeg56wI5fi\nqDurMVXIPJZN6xizrl2G6upukGnjSWbHn6H9hfFxq9czLl5Q3TeTs3slMzOq01YounB3x0a+MXdI\nNWd16zxFTuZoH2Ji3Jr9INquVvfeizr/OzTHpR0bJcj6bVc+hbTO69qVlU55htKEA0CdTLU+xXqg\nfQ7WkZPEp8LubYrr1Q6wujY/LwfivTc58fha/a/daxyDIL7xIyJGEHHhR0SMIIYcpAO0eogkDRK9\n0l3BMfz7RBxqLk2WCcFwcrQX2zvneHmbrp126bVSTTbFqYhdcOa2MpNtZKwq0aC0WRlHRuIDTK7B\nE1Qw2QKbJvfGrH0wF33VmcomSJWYXThi6opFDZxpUjBPccISh/QzgV2+rOQbjXU1OXrvP+NN525R\nilSmBnEQbm9b/sAipc0ulyw/IT87TZor/zwMYirbD9abztS4/rXsn7kgYd92XWmy+TQ/pPbXHFTD\niG/8iIgRRFz4EREjiLjwIyJGEMMn22zrdL0dGrsj2tgExrz6njAxT6athnMhzeU1cs+kUva580Jv\n8ooW69a0o1AoWEKQLLnUhrL9puzey66xe9cm0guKDGw401M+r9/TE5PyV9smLv2MY/PIEblJ2qXQ\nZn7QTE7bNZ2rLLsY7zozXZWIOLidTw3Oenw2Z8fIujATrtZdvsBsQ+fDk7PUyHU4TffTk6xmzL3o\nzYnfD+Zx8Z695pG2973FZPgc7ee3n1J9xnHAvNnxjR8RMYKICz8iYgQxVFE/BBWzfZriApFBiDd3\nkPeVUH7qLv5z4Qi/3nU5El8zzmTHImXW1TVYbKR03SkXnZcjUT/n6ozI6jj3ecwJyew+lTeLwBNT\n1ovt6op6uCVMaOLmqlxRUdxzsTM5yeSEmv2yOatysCqxvWXJSErkKWjyDLhrMelKxon6g3rF8TyO\njbtcBePzei2Oyqxbs19o0LFTA1J9zHTd5uBrY/RupWQudFX8PY1J0zU0on/K113LP7/vcLow0Btf\nRGZE5I9E5Fsi8ryIvE1E5kTkcyLyYvv/7PV7ioiIeCVgUFH/NwD8eQjhddhLp/U8gA8DeDKEcD+A\nJ9vHERERrwIMki13GsD3AviXABBCqAOoi8j7Abyz3ezjAL4A4EP9ewud4Ih02l06xaK4DbARkl+Y\naMGL4izK+R1iFpOMNcAFhnDwRtN5xfHudIV46by6kCPevnzeipSVKqk4XmSl3WoWFVNpK17y/NQd\nxxyn6OILJHX7Xao0/uDopFv0PYv0XWpOPF4j3j4ffMOek8yv6IkymHewUbPqH+/4Mzz9uhAHYXry\nmKkbm1jQMdGcNqrW+69e0u+S7FpOP6MWdG21h31K/SFOhO/pVdpFCNJ7GAel4BvkjX8PgCsAfltE\nviYi/72dLnsxhHAtTOwy9rLqRkREvAowyMLPAHgLgN8KIbwZQAlOrA97P6X7/uCJyOMi8rSIPN1r\nIyQiImK4GGThLwNYDiE81T7+I+z9EKyIyHEAaP9f3e/kEMITIYSHQwgP+zj7iIiIw8F1dfwQwmUR\nOS8irw0hfBvAewA81/77IIBfbv//9CAXFNl/8TfJ067q6lgnygUiynD7BMb7Sqwe2CQyiDSNwe81\npEmv3Nm2vPpsduGoNU/qwFa6lPOKawbSrZ2MxBIRq32eXIKj+jYpVXW7133H681LrOPXqjZyr2U8\n8nQOfNpwJjHx6cBNFCXtqUgf7zOv0zJTiYmCSzui1jE1KOUnj5u68VnV+TndWL3qPA131OxXzZ03\ndfVtJRVtlq3ZstXaP+dDX5Xbm5p7VEmfCEIvXw+clquNQe34/xbA78pegvQzAP4V9qSFT4rIYwDO\nAvjAga4cERFxaBho4YcQvg7g4X2q3nNrhxMRETEMDD1Ip2OH6DJhMLmEDbBhWSgQAQanwgJc5lLv\nOEWiP5NVBGeyqxP3ujfnMSccexA2nZSbo1Re9Yw1TTKvXuKIRHgLhL31Mi4HgeGOcwErg4IDobo9\nJbXM6odP12W9I/uJmr0DT4x3Xh8CDPZ4RM5mCM6MTXbK+aL1IxubVJKRLJkm63XbLlNQD8h0zgb6\nsJdmJbVk6sKuZhb2Hn8M6TM/7DnJmnCqD5mHN8EObEvs9B0RETFyiAs/ImIEERd+RMQIYsjReQGt\ntpLnVRLjtuhMfqz7sTUoNL1OpV8nm7d6cYPIJltN0uNTto8G5ctrOP05Jdp/Lq+6u7O2IUlovM5c\naFM1e452akfnedfVOrm2eoJKsx/Sz8TT2zI0MA5qQtoPbLIS5+dhTFtMYFKwEXiZgur4aa//F9Sd\nN5dX3T2dte2CqKnSm5xTlGshuAmvkDmvWSJX3y6mGb4xvR3ZbCCgn49Wj4YHvxfxjR8RMYKICz8i\nYgQht0JcG/hiIlew5+yzAODqdZrfbrwSxgDEcXjEcVgcdBx3hxCOXK/RUBd+56IiT4cQ9nMIGqkx\nxHHEcRzWOKKoHxExgogLPyJiBHFYC/+JQ7ou45UwBiCOwyOOw+K2jONQdPyIiIjDRRT1IyJGEENd\n+CLyPhH5toicFpGhsfKKyMdEZFVEnqHPhk4PLiJ3isjnReQ5EXlWRH72MMYiIgUR+ZKIfKM9jl9q\nf36PiDzVvj9/0OZfuO0QkXSbz/GzhzUOEVkSkb8Xka+LyNPtzw7jGRkKlf3QFr7sZbT4TQA/AOAB\nAD8uIg8M6fK/A+B97rPDoAdPAPx8COEBAI8A+Jn2HAx7LDUA7w4hvAnAQwDeJyKPAPgVAL8WQrgP\nwAaAx27zOK7hZ7FH2X4NhzWOd4UQHiLz2WE8I8Ohsg8hDOUPwNsA/AUdfwTAR4Z4/VMAnqHjbwM4\n3i4fB/DtYY2FxvBpAO89zLEAKAL4KoDvwZ6jSGa/+3Ubr3+y/TC/G8BnseeFfhjjWAKw4D4b6n0B\nMA3gZbT33m7nOIYp6t8BgMnMltufHRYOlR5cRE4BeDOApw5jLG3x+uvYI0n9HICXAGyG0GEHGdb9\n+XUAvwB00g/PH9I4AoC/FJGviMjj7c+GfV+GRmUfN/fQnx78dkBEJgD8MYCfCyGYzA7DGksIoRlC\neAh7b9y3Anjd7b6mh4j8MIDVEMJXhn3tffBoCOEt2FNFf0ZEvpcrh3RfborK/iAY5sK/AOBOOj7Z\n/uywMBA9+K2GiGSxt+h/N4TwJ4c5FgAIIWwC+Dz2ROoZkU7s8TDuzzsA/IiILAH4BPbE/d84hHEg\nhHCh/X8VwKew92M47PtyU1T2B8EwF/6XAdzf3rHNAfgxAJ8Z4vU9PoM9WnDgAPTgNwPZI5H7KIDn\nQwi/elhjEZEjIjLTLo9hb5/heez9APzosMYRQvhICOFkCOEU9p6H/xNC+Mlhj0NExkVk8loZwPcD\neAZDvi8hhMsAzovIa9sfXaOyv/XjuN2bJm6T4gcBvIA9ffI/DvG6vw/gEoAG9n5VH8OeLvkkgBcB\n/G8Ac0MYx6PYE9O+CeDr7b8fHPZYAHwngK+1x/EMgP/U/vw1AL4E4DSAPwSQH+I9eieAzx7GONrX\n+0b779lrz+YhPSMPAXi6fW/+F4DZ2zGO6LkXETGCiJt7EREjiLjwIyJGEHHhR0SMIOLCj4gYQcSF\nHxExgogLPyJiBBEXfkTECCIu/IiIEcT/BzXEcHTGyJuLAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "VeaXejrGVcqH",
        "colab_type": "code",
        "outputId": "91f944e5-5d2f-44e7-d8d0-6053eb951ef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "m_train = train_set_x_orig.shape[0]\n",
        "m_test = test_set_x_orig.shape[0]\n",
        "num_px = train_set_x_orig.shape[1] \n",
        "\n",
        "\n",
        "print (\"Number of training examples: m_train = \" + str(m_train))\n",
        "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
        "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
        "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: m_train = 209\n",
            "Number of testing examples: m_test = 50\n",
            "Height/Width of each image: num_px = 64\n",
            "Each image is of size: (64, 64, 3)\n",
            "train_set_x shape: (209, 64, 64, 3)\n",
            "train_set_y shape: (1, 209)\n",
            "test_set_x shape: (50, 64, 64, 3)\n",
            "test_set_y shape: (1, 50)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OwC8Q20gaAJP",
        "colab_type": "code",
        "outputId": "852c12e6-19a9-40bc-e678-95a692eefc66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "# Reshape the training and test examples\n",
        "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
        "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
        "\n",
        "\n",
        "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
        "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_set_x_flatten shape: (12288, 209)\n",
            "train_set_y shape: (1, 209)\n",
            "test_set_x_flatten shape: (12288, 50)\n",
            "test_set_y shape: (1, 50)\n",
            "sanity check after reshaping: [17 31 56 22 33]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TKeLoyJoZz6F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2 - Data Normalization"
      ]
    },
    {
      "metadata": {
        "id": "mtWjNtWmUrRE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_set_x = train_set_x_flatten / 255.\n",
        "test_set_x = test_set_x_flatten / 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "brYHPaUEbXGn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3 - Sigmoid Fuction . \n",
        "**Exercise**: Implement `sigmoid()`. As you've seen in the figure above, you need to compute $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ to make predictions. Use `np.exp()`."
      ]
    },
    {
      "metadata": {
        "id": "MvEhCJd0Z4p6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "\n",
        "    ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
        "    s = None\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iBG9ux15b5hp",
        "colab_type": "code",
        "outputId": "9a30043a-3e0b-490c-ef24-4d866c956a40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sigmoid([0, 2]) = None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VO1BhDeZcG2C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Expected Output**: `[ 0.5      0.88079708]`"
      ]
    },
    {
      "metadata": {
        "id": "fy8PRjZbcgfP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.4 - Initializing Parameters\n",
        "**Exercise:** Implement parameter initialization in the cell below. You have to initialize $w$ as a vector of zeros. If you don't know what numpy function to use, look up `np.zeros()` in the Numpy library's documentation."
      ]
    },
    {
      "metadata": {
        "id": "2_NjqOywcEGV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "    \n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "    \n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ### (â‰ˆ 2 line of code)\n",
        "    w = None\n",
        "    b = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(w.shape == (dim, 1))\n",
        "    assert(isinstance(b, float) or isinstance(b, int))\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZdCK1yhZcyN5",
        "colab_type": "code",
        "outputId": "aff73a1d-160d-457c-879c-5459ae9eb947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        }
      },
      "cell_type": "code",
      "source": [
        "dim = 2\n",
        "w, b = initialize_with_zeros(dim)\n",
        "print (\"w = \" + str(w))\n",
        "print (\"b = \" + str(b))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-def32b236a24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_with_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"w = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"b = \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-d7defc399d90>\u001b[0m in \u001b[0;36minitialize_with_zeros\u001b[0;34m(dim)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "HFzO6n_5c1qg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Expected Output**:  \n",
        "`w = [[0.]\n",
        " [0.]]`  \n",
        " \n",
        "`b = 0`"
      ]
    },
    {
      "metadata": {
        "id": "rBogHhb9dNu4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.5 - Forward Propagate . \n",
        "**Exercise:** Implement a function `propagate()` that computes the cost function and its gradient.\n",
        "\n",
        "**Hints**:\n",
        "\n",
        "Forward Propagation:\n",
        "- You get X\n",
        "- You compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
        "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$ \n",
        "\n",
        "Here are the two formulas you will be using: \n",
        "\n",
        "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
        "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$  \n",
        "\n",
        "Try `np.log()` if you need."
      ]
    },
    {
      "metadata": {
        "id": "SIP3BINWcz1L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
        "\n",
        "    Return:\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
        "    db -- gradient of the loss with respect to b, thus same shape as b\n",
        "    \n",
        "    Tips:\n",
        "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    \n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
        "    A = None                                 # compute activation\n",
        "    cost = None                              # compute cost\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
        "    dw = None\n",
        "    db = None\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    assert(dw.shape == w.shape)\n",
        "    assert(db.dtype == float)\n",
        "    cost = np.squeeze(cost)\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return grads, cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KMdWA4gsdpbF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZQyKxrddeC1x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Expected Output**:    \n",
        "`dw = [[ 0.99845601]\n",
        " [ 2.39507239]]`  \n",
        "`db = 0.00145557813678`  \n",
        "`cost = 5.801545319394553`"
      ]
    },
    {
      "metadata": {
        "id": "AkEJihI-fFTZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.6 - Optimization\n",
        "- You have initialized your parameters.\n",
        "- You are also able to compute a cost function and its gradient.\n",
        "- Now, you want to update the parameters using gradient descent.\n",
        "\n",
        "**Exercise:** Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
      ]
    },
    {
      "metadata": {
        "id": "pcCmSzaIfHYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "    \n",
        "    Tips:\n",
        "    You basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "    \n",
        "    costs = []\n",
        "    \n",
        "    for i in range(num_iterations):\n",
        "        \n",
        "        \n",
        "        # Cost and gradient calculation (â‰ˆ 1-4 lines of code)\n",
        "        ### START CODE HERE ### \n",
        "        grads, cost = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        \n",
        "        # update rule (â‰ˆ 2 lines of code)\n",
        "        ### START CODE HERE ###\n",
        "        w = None\n",
        "        b = None\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        \n",
        "        # Print the cost every 100 training iterations\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    \n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    \n",
        "    return params, grads, costs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LLdRVNglfhHL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aEKl53j2fi1K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Expected Output**:    \n",
        "`w = [[ 0.19033591]\n",
        " [ 0.12259159]]`  \n",
        "`b = 1.92535983008`  \n",
        "`dw = [[ 0.67752042]\n",
        " [ 1.41625495]]`  \n",
        "`db = 0.219194504541` . "
      ]
    },
    {
      "metadata": {
        "id": "yc0f4Bc_f80L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Exercise:** The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the `predict()` function. There are two steps to computing predictions:\n",
        "\n",
        "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
        "\n",
        "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. If you wish, you can use an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this). "
      ]
    },
    {
      "metadata": {
        "id": "U_TpYwHFftmA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "    \n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    \n",
        "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
        "    ### START CODE HERE ### (â‰ˆ 1 line of code)\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    for i in range(A.shape[1]):\n",
        "        \n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
        "        ### START CODE HERE ### (â‰ˆ 4 lines of code)\n",
        "        None\n",
        "            \n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "    assert(Y_prediction.shape == (1, m))\n",
        "    \n",
        "    return Y_prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-MxGAz1TgC6C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w = np.array([[0.1124579],[0.23106775]])\n",
        "b = -0.3\n",
        "X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])\n",
        "print (\"predictions = \" + str(predict(w, b, X)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IjnYN1_2gHAl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Expected Output**:  \n",
        "`predictions = [[ 1.  1.  0.]]`"
      ]
    },
    {
      "metadata": {
        "id": "olBDvIBlgT-K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4 - Merge All Functions into a Model ##\n",
        "\n",
        "You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
        "\n",
        "**Exercise:** Implement the model function. Use the following notation:\n",
        "    - Y_prediction_test for your predictions on the test set\n",
        "    - Y_prediction_train for your predictions on the train set\n",
        "    - w, costs, grads for the outputs of optimize()"
      ]
    },
    {
      "metadata": {
        "id": "S1_mqmqpgRZH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "    \n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to true to print the cost every 100 iterations\n",
        "    \n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # initialize parameters with zeros (â‰ˆ 1 line of code)\n",
        "    w, b = None\n",
        "\n",
        "    # Gradient descent (â‰ˆ 1 line of code)\n",
        "    # optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False)\n",
        "    parameters, grads, costs = None\n",
        "    \n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    \n",
        "    # Predict test/train set examples (â‰ˆ 2 lines of code)\n",
        "    Y_prediction_test = None\n",
        "    Y_prediction_train = None\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # Print train/test Errors\n",
        "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "    \n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test, \n",
        "         \"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    \n",
        "    return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LvMQdgOXhGc6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_BE-2szhhtQG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot learning curve (with costs)\n",
        "costs = np.squeeze(d['costs'])\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "45XoKJMohKt3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.1 - Choice of Learning Rate\n"
      ]
    },
    {
      "metadata": {
        "id": "dpXBNrKah3kF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "models = {}\n",
        "for i in learning_rates:\n",
        "    print (\"learning rate is: \" + str(i))\n",
        "    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
        "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
        "\n",
        "for i in learning_rates:\n",
        "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
        "\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (hundreds)')\n",
        "\n",
        "legend = plt.legend(loc='upper center', shadow=True)\n",
        "frame = legend.get_frame()\n",
        "frame.set_facecolor('0.90')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B3bSBDD-iJxA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4.2 - Test with your own image"
      ]
    },
    {
      "metadata": {
        "id": "TL3436q3h34s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "my_image = \"my_image.jpg\"   # change this to the name of your image file \n",
        "\n",
        "\n",
        "# We preprocess the image to fit your algorithm.\n",
        "fname = my_image\n",
        "image = np.array(ndimage.imread(fname, flatten=False))\n",
        "my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\n",
        "my_predicted_image = predict(d[\"w\"], d[\"b\"], my_image)\n",
        "\n",
        "plt.imshow(image)\n",
        "print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}